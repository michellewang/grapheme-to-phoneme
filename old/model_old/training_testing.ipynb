{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFhg-cOfqa3W",
        "colab_type": "code",
        "outputId": "9c9fc643-9c54-4d6e-9d0d-7f638cc840ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# TODO: test on training data to see if model is overfitting\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_dyB1V_O5b6",
        "colab_type": "code",
        "outputId": "a778dc14-0a4a-4d9d-8198-d533ff7a61b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd gdrive/My Drive/University/MAIS202/Final Project"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/University/MAIS202/Final Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKg_tkD8QUfT",
        "colab_type": "code",
        "outputId": "d76a07cf-6e65-4017-fe89-c64a334dd657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import pickle\n",
        "\n",
        "import itertools as it\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "from keras.layers.merge import Concatenate\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "import datetime"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6BUOZNKPuqQ",
        "colab_type": "code",
        "outputId": "c1e0b20d-c9a8-4a45-d826-43ff7cbf4b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "with open('data_preprocessed.pickle', 'rb') as file_data:\n",
        "  char_input = pickle.load(file_data)\n",
        "  \n",
        "  phoneme_input = char_input['phonemes']\n",
        "  char_input = char_input['chars']\n",
        "  \n",
        "print(char_input.shape)\n",
        "print(phoneme_input.shape)\n",
        "\n",
        "max_phoneme_seq_len = phoneme_input.shape[1]\n",
        "\n",
        "num_char_tokens = char_input.shape[2]\n",
        "num_phoneme_tokens = phoneme_input.shape[2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(124558, 20, 26)\n",
            "(124558, 21, 71)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1fYtiprZXKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('mappings.pickle', 'rb') as file_mappings:\n",
        "  map_char_id = pickle.load(file_mappings)\n",
        "  \n",
        "  map_id_char = map_char_id['id_char']\n",
        "  map_phoneme_id = map_char_id['phoneme_id']\n",
        "  map_id_phoneme = map_char_id['id_phoneme']\n",
        "  map_char_id = map_char_id['char_id']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RZ9At5ecE2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data_dict.pickle', 'rb') as file_dict:\n",
        "  data_dict = pickle.load(file_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXlFbjOZQ1Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phoneme_output = np.pad(phoneme_input,((0,0),(0,1),(0,0)), mode='constant')[:,1:,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeBaWqstTo53",
        "colab_type": "code",
        "outputId": "b1cae7eb-c3ab-4e90-d0ec-ad2e69eac11e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "(char_input_train, char_input_test, \n",
        " phoneme_input_train, phoneme_input_test, \n",
        " phoneme_output_train, phoneme_output_test) = train_test_split(\n",
        "    char_input, phoneme_input, phoneme_output, test_size=0.2, random_state=3791)\n",
        "\n",
        "print(char_input_train.shape, char_input_test.shape)\n",
        "print(phoneme_input_train.shape, phoneme_input_test.shape)\n",
        "print(phoneme_output_train.shape, phoneme_output_test.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(99646, 20, 26) (24912, 20, 26)\n",
            "(99646, 21, 71) (24912, 21, 71)\n",
            "(99646, 21, 71) (24912, 21, 71)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tkukw_rUlkhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_models(encoder_type, decoder_type, hidden_nodes_encoder):\n",
        "  \n",
        "  ### encoder\n",
        "  char_inputs = Input(shape=(None, num_char_tokens))\n",
        "  \n",
        "  if encoder_type == 'lstm':\n",
        "    encoder = LSTM(hidden_nodes_encoder, return_state=True)\n",
        "    hidden_nodes_decoder = hidden_nodes_encoder\n",
        "  elif encoder_type == 'blstm':\n",
        "    encoder = Bidirectional(LSTM(hidden_nodes_encoder, return_state=True))\n",
        "    hidden_nodes_decoder = 2*hidden_nodes_encoder\n",
        "\n",
        "  ### decoder\n",
        "  phoneme_inputs = Input(shape=(None, num_phoneme_tokens))\n",
        "  if decoder_type == 'lstm':\n",
        "    decoder = LSTM(hidden_nodes_decoder, return_sequences=True, return_state=True)\n",
        "  decoder_dense = Dense(num_phoneme_tokens, activation='softmax')\n",
        "  \n",
        "  ### training model\n",
        "  if encoder_type == 'lstm':\n",
        "    _, state_h, state_c = encoder(char_inputs) # ignore encoder outputs\n",
        "  elif encoder_type == 'blstm':\n",
        "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder(char_inputs) # ignore encoder outputs\n",
        "    # concatenate states of forward and backward LSTMs\n",
        "    state_h = Concatenate()([forward_h, backward_h])\n",
        "    state_c = Concatenate()([forward_c, backward_c])\n",
        "  encoder_states = [state_h, state_c]\n",
        "  \n",
        "  # initialize decoder with encoder states (hidden and internal)\n",
        "  decoder_outputs, _, _ = decoder(phoneme_inputs, initial_state=encoder_states)\n",
        "  phoneme_prediction = decoder_dense(decoder_outputs)\n",
        "   \n",
        "  training_model = Model([char_inputs, phoneme_inputs], phoneme_prediction)\n",
        "  \n",
        "  ### testing models\n",
        "  # encoder\n",
        "  testing_encoder_model = Model(char_inputs, encoder_states)\n",
        "\n",
        "  # decoder\n",
        "  decoder_state_input_h = Input(shape=(hidden_nodes_decoder,)) # hidden state\n",
        "  decoder_state_input_c = Input(shape=(hidden_nodes_decoder,)) # internal state\n",
        "  decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "  decoder_outputs, decoder_state_h, decoder_state_c = decoder(phoneme_inputs, initial_state=decoder_state_inputs) \n",
        "  decoder_states = [decoder_state_h, decoder_state_c]\n",
        "  phoneme_prediction = decoder_dense(decoder_outputs)\n",
        "  \n",
        "  testing_decoder_model = Model([phoneme_inputs] + decoder_state_inputs, [phoneme_prediction] + decoder_states)\n",
        "  \n",
        "  return training_model, testing_encoder_model, testing_decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNPxw9d4pf3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, path_weights, encoder_input, decoder_input, decoder_output, patience, optimizer, batch_size):\n",
        "  \n",
        "    # save weights after training epochs\n",
        "    checkpointer = ModelCheckpoint(filepath=path_weights, verbose=1, save_best_only=True)\n",
        "    \n",
        "    # stop training if validation loss does not decrease after some consecutive epochs\n",
        "    stopper = EarlyStopping(monitor='val_loss',patience=patience)\n",
        "    \n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "    train_history = model.fit([encoder_input, decoder_input], decoder_output,\n",
        "                               batch_size=batch_size,\n",
        "                               epochs=100,\n",
        "                               validation_split=0.125,\n",
        "                               callbacks=[checkpointer, stopper])\n",
        "    \n",
        "    return train_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOvNpwDtug83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(input_char_seq, encoder, decoder):\n",
        "  '''Predicts the pronunciation of one word'''\n",
        "  \n",
        "  # convert input to 3D array if necessary\n",
        "  if len(input_char_seq.shape) == 2:\n",
        "    input_char_seq = np.array([input_char_seq])\n",
        "  \n",
        "  start_token = '\\t'\n",
        "  end_token = '\\n'\n",
        "\n",
        "  # feed input to encoder and get encoder state vectors\n",
        "  state_vectors = encoder.predict(input_char_seq) \n",
        "\n",
        "  # create start token\n",
        "  prev_phoneme = np.zeros((1, 1, num_phoneme_tokens))\n",
        "  prev_phoneme[0, 0, map_phoneme_id[start_token]] = 1.\n",
        "\n",
        "  end_found = False\n",
        "  pronunciation = ''\n",
        "  \n",
        "  while not end_found:\n",
        "    \n",
        "    # feed encoder states and token ('start' or previous prediction)\n",
        "    decoder_output, h, c = decoder.predict([prev_phoneme] + state_vectors)\n",
        "\n",
        "    # get phoneme with the highest probability\n",
        "    predicted_phoneme_id = np.argmax(decoder_output[0, -1, :])\n",
        "    predicted_phoneme = map_id_phoneme[predicted_phoneme_id]\n",
        "\n",
        "    # add predicted phoneme to sequence\n",
        "    pronunciation += predicted_phoneme + ' '\n",
        "\n",
        "    if predicted_phoneme == end_token or len(pronunciation.split()) > max_phoneme_seq_len: \n",
        "        end_found = True\n",
        "\n",
        "    # use predicted phoneme and decoder states as input in the next iteration\n",
        "    prev_phoneme = np.zeros((1, 1, num_phoneme_tokens))\n",
        "    prev_phoneme[0, 0, predicted_phoneme_id] = 1.\n",
        "    state_vectors = [h, c]\n",
        "\n",
        "  # return string of phonemes\n",
        "  return pronunciation.strip()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEPTGVCl2Pr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def matrix_to_word(char_matrix):\n",
        "  \n",
        "  word = ''\n",
        "  \n",
        "  for char_vector in char_matrix:\n",
        "    \n",
        "    # stop if vector only contains 0s\n",
        "    if np.count_nonzero(char_vector) == 0:\n",
        "      break\n",
        "    \n",
        "    word += map_id_char[char_vector.argmax()]\n",
        "    \n",
        "  return word\n",
        "\n",
        "def is_correct(word, prediction):\n",
        "  \n",
        "  # get correct pronunciations from dictionary\n",
        "  correct_pronunciations = data_dict[word]\n",
        "  \n",
        "  for correct in correct_pronunciations:\n",
        "    if prediction == correct:\n",
        "      return True\n",
        "  \n",
        "  return False\n",
        "\n",
        "def bleu_score(word, test_pronunciation):\n",
        "    references = [pronun.split() for pronun in data_dict[word]]\n",
        "    smooth = SmoothingFunction().method1\n",
        "    return sentence_bleu(references, test_pronunciation.split(), smoothing_function=smooth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JnuakC8_kfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(samples, encoder, decoder):\n",
        "  \n",
        "  n_samples = len(samples)\n",
        "  \n",
        "  print('Testing {} samples'.format(n_samples))\n",
        "  \n",
        "  correct = []\n",
        "  bleu = []\n",
        "  for i, sample in enumerate(samples):\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "      if i%10000 == 0:\n",
        "        print()\n",
        "      print(i, end='...')\n",
        "      \n",
        "    word = matrix_to_word(sample)\n",
        "    prediction = predict(sample, encoder, decoder)\n",
        "      \n",
        "    correct.append(is_correct(word, prediction))\n",
        "    bleu.append(bleu_score(word, prediction))\n",
        "      \n",
        "  print()\n",
        "      \n",
        "  acc = np.mean(correct)\n",
        "  avg_bleu = np.mean(bleu)\n",
        "  \n",
        "  print('Accuracy:', acc)\n",
        "  print('Average BLEU score:', avg_bleu)\n",
        "  \n",
        "  return acc, avg_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh_GjN2u80-u",
        "colab_type": "code",
        "outputId": "a488e0e0-a0cf-4b27-ff65-54036e4abdc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "encoder_type_all = ['blstm']\n",
        "decoder_type_all = ['lstm']\n",
        "n_hidden_nodes_all = [512, 256]\n",
        "patience_all = [1, 2, 3]\n",
        "optimizer_all = ['adam']\n",
        "batch_size_all = [32]\n",
        "\n",
        "# encoder_type_all = ['blstm']\n",
        "# decoder_type_all = ['lstm']\n",
        "# n_hidden_nodes_all = [512]\n",
        "# patience_all = [3]\n",
        "\n",
        "hyperparameters_all = it.product(encoder_type_all, decoder_type_all, n_hidden_nodes_all, patience_all, optimizer_all, batch_size_all)\n",
        "\n",
        "\n",
        "for hyperparameters in hyperparameters_all:\n",
        "  \n",
        "  start = datetime.datetime.utcnow()\n",
        "  \n",
        "  print('Hyperparameters:', hyperparameters)\n",
        "  \n",
        "  if hyperparameters in [('lstm', 'lstm', 256, 1, 'adam', 64), \n",
        "                         ('lstm', 'lstm', 256, 1, 'adam', 128),\n",
        "                         ('lstm', 'lstm', 256, 1, 'adam', 256), \n",
        "                         ('lstm', 'lstm', 256, 1, 'rmsprop', 64), \n",
        "                         ('lstm', 'lstm', 256, 1, 'rmsprop', 128), \n",
        "                         ('lstm', 'lstm', 256, 1, 'rmsprop', 256),\n",
        "                         ('lstm', 'lstm', 256, 2, 'adam', 64), \n",
        "                         ('lstm', 'lstm', 256, 2, 'adam', 128),\n",
        "                         ('blstm', 'lstm', 256, 1, 'adam', 64), \n",
        "                         ('blstm', 'lstm', 256, 1, 'adam', 128), \n",
        "                         ('blstm', 'lstm', 256, 1, 'adam', 256), \n",
        "                         ('blstm', 'lstm', 256, 1, 'rmsprop', 64),\n",
        "                         ('blstm', 'lstm', 256, 1, 'rmsprop', 128), \n",
        "                         ('blstm', 'lstm', 256, 1, 'rmsprop', 256), \n",
        "                         ('blstm', 'lstm', 256, 2, 'adam', 64), \n",
        "                         ('blstm', 'lstm', 512, 1, 'adam', 64),\n",
        "                         ('blstm', 'lstm', 512, 1, 'adam', 128), \n",
        "                         ('blstm', 'lstm', 512, 1, 'adam', 256),\n",
        "                         ('blstm', 'lstm', 512, 3, 'adam', 64),\n",
        "                         ('blstm', 'lstm', 512, 3, 'adam', 128),\n",
        "                         ('blstm', 'lstm', 512, 3, 'adam', 256),\n",
        "                         ('blstm', 'lstm', 512, 2, 'adam', 64),\n",
        "                         ('blstm', 'lstm', 512, 2, 'adam', 128), \n",
        "                         ('blstm', 'lstm', 512, 2, 'adam', 256),\n",
        "                         ('blstm', 'lstm', 256, 3, 'adam', 64),\n",
        "                         ('blstm', 'lstm', 256, 3, 'adam', 128),\n",
        "                         ('blstm', 'lstm', 256, 3, 'adam', 256),\n",
        "                         ('blstm', 'lstm', 256, 2, 'adam', 128),\n",
        "                         ('blstm', 'lstm', 256, 2, 'adam', 256),\n",
        "                         ('blstm', 'lstm', 512, 1, 'adam', 32),\n",
        "                         ('blstm', 'lstm', 512, 2, 'adam', 32),\n",
        "                         ('blstm', 'lstm', 512, 3, 'adam', 32),\n",
        "                         ('blstm', 'lstm', 256, 1, 'adam', 32),\n",
        "                         ('blstm', 'lstm', 256, 2, 'adam', 32),\n",
        "                         ('blstm', 'lstm', 256, 3, 'adam', 32)]:\n",
        "    print('Skipping this set of hyperparamters')\n",
        "    continue\n",
        "  \n",
        "  encoder_type, decoder_type, n_hidden_nodes, patience, optimizer, batch_size = hyperparameters\n",
        "  hyperparameters_string = '{}_{}_{}_{}_{}_{}'.format(encoder_type, decoder_type, n_hidden_nodes, patience, optimizer, batch_size)\n",
        "  \n",
        "  path_weights = '{}_weights.hdf5'.format(hyperparameters_string)\n",
        "  print('File name:', path_weights)\n",
        "  \n",
        "  training_model, testing_encoder_model, testing_decoder_model = create_models(encoder_type, decoder_type, n_hidden_nodes)\n",
        "  \n",
        "  train_history = train(training_model, path_weights, char_input_train, phoneme_input_train, phoneme_output_train, patience, optimizer, batch_size)\n",
        "  \n",
        "#   print(train_history.history['loss'])\n",
        "#   print(train_history.history['val_loss'])\n",
        "\n",
        "  training_model.load_weights(path_weights)\n",
        "  acc, bleu = test(char_input_test, testing_encoder_model, testing_decoder_model)\n",
        "  \n",
        "  path_metrics = '{}_metrics.pickle'.format(hyperparameters_string)\n",
        "  \n",
        "  with open(path_metrics, 'wb') as file_out:\n",
        "    \n",
        "    pickle.dump({'history':train_history, 'acc':acc, 'bleu':bleu}, file_out)\n",
        "    \n",
        "    print('History, accuracy and average BLEU score saved to {}\\n'.format(path_metrics))\n",
        "    \n",
        "  with open('training_log.txt', 'a+') as file_out:\n",
        "    \n",
        "    time = datetime.datetime.utcnow() - start\n",
        "    \n",
        "    file_out.write('\\nHyperparameters: {}\\n'.format(hyperparameters))\n",
        "    file_out.write('\\tAccuracy: {}\\n'.format(acc))\n",
        "    file_out.write('\\tAverage BLEU score: {}\\n'.format(bleu))\n",
        "    file_out.write('\\tTotal training and testing time: {}\\n'.format(time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyperparameters: ('blstm', 'lstm', 512, 1, 'adam', 32)\n",
            "Skipping this set of hyperparamters\n",
            "Hyperparameters: ('blstm', 'lstm', 512, 2, 'adam', 32)\n",
            "Skipping this set of hyperparamters\n",
            "Hyperparameters: ('blstm', 'lstm', 512, 3, 'adam', 32)\n",
            "Skipping this set of hyperparamters\n",
            "Hyperparameters: ('blstm', 'lstm', 256, 1, 'adam', 32)\n",
            "Skipping this set of hyperparamters\n",
            "Hyperparameters: ('blstm', 'lstm', 256, 2, 'adam', 32)\n",
            "File name: blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 87190 samples, validate on 12456 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "87190/87190 [==============================] - 300s 3ms/step - loss: 0.3573 - val_loss: 0.1803\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.18033, saving model to blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "Epoch 2/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.1442 - val_loss: 0.1352\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.18033 to 0.13517, saving model to blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "Epoch 3/100\n",
            "87190/87190 [==============================] - 290s 3ms/step - loss: 0.1125 - val_loss: 0.1175\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.13517 to 0.11751, saving model to blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "Epoch 4/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.0945 - val_loss: 0.1122\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.11751 to 0.11218, saving model to blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "Epoch 5/100\n",
            "87190/87190 [==============================] - 293s 3ms/step - loss: 0.0815 - val_loss: 0.1066\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.11218 to 0.10656, saving model to blstm_lstm_256_2_adam_32_weights.hdf5\n",
            "Epoch 6/100\n",
            "87190/87190 [==============================] - 294s 3ms/step - loss: 0.0708 - val_loss: 0.1072\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.10656\n",
            "Epoch 7/100\n",
            "87190/87190 [==============================] - 294s 3ms/step - loss: 0.0621 - val_loss: 0.1070\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.10656\n",
            "Testing 24912 samples\n",
            "0...1000...2000...3000...4000...5000...6000...7000...8000...9000...10000...11000...12000...13000...14000...15000...16000...17000...18000...19000...20000...21000...22000...23000...24000...\n",
            "Accuracy: 0.5865446371226718\n",
            "Average BLEU score: 0.7122090929742386\n",
            "History, accuracy and average BLEU score saved to blstm_lstm_256_2_adam_32_metrics.pickle\n",
            "\n",
            "Hyperparameters: ('blstm', 'lstm', 256, 3, 'adam', 32)\n",
            "File name: blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Train on 87190 samples, validate on 12456 samples\n",
            "Epoch 1/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.3465 - val_loss: 0.1802\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.18019, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 2/100\n",
            "87190/87190 [==============================] - 294s 3ms/step - loss: 0.1441 - val_loss: 0.1327\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.18019 to 0.13269, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 3/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.1120 - val_loss: 0.1171\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.13269 to 0.11710, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 4/100\n",
            "87190/87190 [==============================] - 299s 3ms/step - loss: 0.0945 - val_loss: 0.1127\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.11710 to 0.11271, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 5/100\n",
            "87190/87190 [==============================] - 297s 3ms/step - loss: 0.0812 - val_loss: 0.1100\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.11271 to 0.10999, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 6/100\n",
            "87190/87190 [==============================] - 293s 3ms/step - loss: 0.0710 - val_loss: 0.1078\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.10999 to 0.10777, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 7/100\n",
            "87190/87190 [==============================] - 292s 3ms/step - loss: 0.0621 - val_loss: 0.1063\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.10777 to 0.10633, saving model to blstm_lstm_256_3_adam_32_weights.hdf5\n",
            "Epoch 8/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.0553 - val_loss: 0.1087\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.10633\n",
            "Epoch 9/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.0493 - val_loss: 0.1109\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.10633\n",
            "Epoch 10/100\n",
            "87190/87190 [==============================] - 291s 3ms/step - loss: 0.0448 - val_loss: 0.1144\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.10633\n",
            "Testing 24912 samples\n",
            "0...1000...2000...3000...4000...5000...6000...7000...8000...9000...10000...11000...12000...13000...14000...15000...16000...17000...18000...19000...20000...21000...22000...23000...24000...\n",
            "Accuracy: 0.6063342967244701\n",
            "Average BLEU score: 0.7250990263625238\n",
            "History, accuracy and average BLEU score saved to blstm_lstm_256_3_adam_32_metrics.pickle\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zVcIiLUTCu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "ecec84db-0d20-491d-d506-741f057eef6c"
      },
      "source": [
        "best_params = ('blstm', 'lstm', 512, 2, 'adam', 32)\n",
        "\n",
        "encoder_type, decoder_type, hidden_nodes, patience, optimizer, batch_size = best_params\n",
        "\n",
        "path_weights = '{}_{}_{}_{}_{}_{}_weights.hdf5'.format(encoder_type, decoder_type, hidden_nodes, patience, optimizer, batch_size)\n",
        "\n",
        "training_model, testing_encoder_model, testing_decoder_model = load_model(\n",
        "  encoder_type, decoder_type, hidden_nodes, patience, optimizer, batch_size, path_weights)\n",
        "\n",
        "print('Test set:')\n",
        "acc_test, bleu_test = test(char_input_test, testing_encoder_model, testing_decoder_model)\n",
        "print()\n",
        "\n",
        "print('Training set:')\n",
        "acc_train, bleu_train = test(char_input_train, testing_encoder_model, testing_decoder_model)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing 24912 samples\n",
            "0...1000...2000...3000...4000...5000...6000...7000...8000...9000...10000...11000...12000...13000...14000...15000...16000...17000...18000...19000...20000...21000...22000...23000...24000...\n",
            "Accuracy: 0.6195407835581246\n",
            "Average BLEU score: 0.7358755330145365\n",
            "Test set:\n",
            "\tAccuracy: 0.6195407835581246\n",
            "\tBLEU score: 0.7358755330145365\n",
            "\n",
            "Testing 99646 samples\n",
            "0...1000...2000...3000...4000...5000...6000...7000...8000...9000...10000...11000...12000...13000...14000...15000...16000...17000...18000...19000...20000...21000...22000...23000...24000...25000...26000...27000...28000...29000...30000...31000...32000...33000...34000...35000...36000...37000...38000...39000...40000...41000...42000...43000...44000...45000...46000...47000...48000...49000...50000...51000...52000...53000...54000...55000...56000...57000...58000...59000...60000...61000...62000...63000...64000...65000...66000...67000...68000...69000...70000...71000...72000...73000...74000...75000...76000...77000...78000...79000...80000...81000...82000...83000...84000...85000...86000...87000...88000...89000...90000...91000...92000...93000...94000...95000...96000...97000...98000...99000...\n",
            "Accuracy: 0.757501555506493\n",
            "Average BLEU score: 0.8273655007660606\n",
            "Training set:\n",
            "\tAccuracy: 0.757501555506493\n",
            "\tBLEU score: 0.8273655007660606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDXCqWbfUThY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(encoder_type, decoder_type, hidden_nodes, patience, optimizer, batch_size, path_weights):\n",
        "  \n",
        "  training_model, testing_encoder_model, testing_decoder_model = create_models(encoder_type, decoder_type, hidden_nodes)\n",
        "  \n",
        "  training_model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
        "  \n",
        "  training_model.load_weights(path_weights)\n",
        "  \n",
        "  return training_model, testing_encoder_model, testing_decoder_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs0AAiymE543",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# acc, bleu = test(char_input_test[:100], testing_encoder_model, testing_decoder_model)\n",
        "# with open(path_metrics, 'wb') as file_out:\n",
        "    \n",
        "#     pickle.dump({'history':train_history, 'acc':acc, 'bleu':bleu}, file_out)\n",
        "    \n",
        "#     print('History, accuracy and average BLEU score saved to {}'.format(path_metrics))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwiDlxqJzhop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count = 0\n",
        "# errors = 0\n",
        "\n",
        "# for i, char_sequence in enumerate(char_input_test):\n",
        "  \n",
        "#   if i % 1000 == 0:\n",
        "#     print(i)\n",
        "  \n",
        "#   count += 1\n",
        "\n",
        "#   word = matrix_to_word(char_sequence)\n",
        "#   prediction = predict(char_sequence, testing_encoder_model, testing_decoder_model)\n",
        "  \n",
        "#   if not is_correct(word, prediction):\n",
        "#     errors += 1\n",
        "# #     print(word)\n",
        "# #     print('\\t', prediction)\n",
        "# #     print('\\t', data_dict[word])\n",
        "  \n",
        "# print('Accuracy:', 1 - errors/count)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}