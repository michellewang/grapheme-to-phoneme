,encoder,layers,rnn_size,dropout,batch_size,patience,optimizer,logfile,best_model,accuracy,bleu
0,rnn,2,500,0.3,64,10,sgd,model0-training-log.txt,model0_steps_96900.pt,0.6573108450281104,80.41
1,rnn,2,500,0.2,32,10,adam,model1-training-log.txt,model1_steps_101960.pt,0.6466246083859062,79.79
2,brnn,2,500,0.2,32,10,adam,model2-training-log.txt,model2_steps_119803.pt,0.6683833311875027,81.45
3,rnn,3,500,0.2,32,10,adam,model3-training-log.txt,model3_steps_150391.pt,0.6488562722629929,79.88
4,brnn,3,500,0.2,32,10,adam,model4-training-log.txt,model4_steps_124901.pt,0.6679970816703146,81.35
5,rnn,2,500,0.3,32,10,adam,model5-training-log.txt,model5_steps_112156.pt,0.6476975237114287,79.81
6,brnn,2,500,0.3,32,10,adam,model6-training-log.txt,model6_steps_114705.pt,0.6670099995708338,81.24
7,rnn,3,500,0.3,32,10,adam,model7-training-log.txt,model7_steps_91764.pt,0.6555083472812325,80.07
8,brnn,3,500,0.3,32,10,adam,model8-training-log.txt,model8_steps_109607.pt,0.6660229174713531,81.17
9,brnn,2,500,0.1,32,10,adam,model9-training-log.txt,model9_steps_127450.pt,0.6621604222994721,81.01
10,brnn,3,500,0.1,32,10,adam,model10-training-log.txt,model10_steps_117254.pt,0.6664520836015622,81.17
11,brnn,4,500,0.1,32,10,adam,model11-training-log.txt,model11_steps_86666.pt,0.6573537616411312,80.67
12,brnn,2,600,0.1,32,10,adam,model12-training-log.txt,model12_steps_129999.pt,0.6629329213338483,81.06
13,brnn,3,600,0.1,32,10,adam,model13-training-log.txt,model13_steps_129999.pt,0.6680829148963564,81.32
14,brnn,4,600,0.1,32,10,adam,model14-training-log.txt,model14_steps_122352.pt,0.6651216685979142,81.26
15,brnn,4,500,0.2,32,10,adam,model15-training-log.txt,model15_steps_150391.pt,0.662632505042702,80.94
16,brnn,2,600,0.2,32,10,adam,model16-training-log.txt,model16_steps_81568.pt,0.6654650015020814,81.13
17,brnn,3,600,0.2,32,10,adam,model17-training-log.txt,model17_steps_150391.pt,0.6680829148963564,81.27
