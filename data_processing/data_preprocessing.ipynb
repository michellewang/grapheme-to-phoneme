{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_word_len = 20\n",
    "start_token = '\\t'\n",
    "end_token = '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(word):\n",
    "    return re.match('[A-Z]*(\\(\\d+\\))?$', word) and len(word) <= max_word_len\n",
    "\n",
    "def process(word):\n",
    "    match = re.search('[A-Z]*', word)\n",
    "    return match.group()\n",
    "\n",
    "def remove_last(word):\n",
    "    return re.sub('(\\(\\d+\\))?$', '', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words before preprocessing: 125074\n",
      "Number of pronunciations before preprocessing: 133854\n",
      "Number of words after preprocessing: 116503\n",
      "Number of pronunciations after preprocessing: 124558\n",
      "\n",
      "Number of samples: 124558\n",
      "Number of unique input tokens (characters): 26\n",
      "Number of unique output tokens (phonemes): 71\n",
      "Max sequence length for inputs: 20\n",
      "Max sequence length for outputs: 21\n"
     ]
    }
   ],
   "source": [
    "# data in dict format\n",
    "data_dict = {} # keys are words and values are lists of pronunciation as strings\n",
    "\n",
    "words_all = set() # for counting the total number of words\n",
    "\n",
    "# data in list format\n",
    "data_chars = [] # each element in the list is a list of characters (representing a word)\n",
    "data_phonemes = [] # each element in the list is a list of phonemes (representing a pronunciation)\n",
    "\n",
    "# these will contain all unique characters/phonemes from the dataset\n",
    "list_characters = set()\n",
    "list_phonemes = set()\n",
    "\n",
    "rejected = set()\n",
    "\n",
    "filename_data = 'cmudict.txt'\n",
    "\n",
    "with open(filename_data, encoding='ISO-8859-1') as data_file:\n",
    "    \n",
    "    total_pronunciations = 0\n",
    "    kept_pronunciations = 0\n",
    "    \n",
    "    for line in data_file:\n",
    "    \n",
    "        # skip comments\n",
    "        if line[:3] == ';;;':\n",
    "            continue\n",
    "        \n",
    "        # split line into word and phonemes\n",
    "        word, phonemes = line.strip().split('  ')\n",
    "        \n",
    "        total_pronunciations += 1\n",
    "        \n",
    "        if remove_last(word) not in words_all:\n",
    "            words_all.add(remove_last(word))\n",
    "        \n",
    "        # check if word is valid\n",
    "        if is_valid(word):\n",
    "            \n",
    "            kept_pronunciations += 1\n",
    "            \n",
    "            # remove (#) ending for words with multiple pronounciations\n",
    "            clean_word = process(word)\n",
    "            \n",
    "            # add new word to dict\n",
    "            if clean_word not in data_dict:\n",
    "                data_dict[clean_word] = []\n",
    "            \n",
    "            # add pronunciation to dict\n",
    "            data_dict[clean_word].append(phonemes)\n",
    "            \n",
    "            # convert phoneme string to list and append special 'start' and 'end' symbols\n",
    "            phonemes = [start_token] + phonemes.split(' ') + [end_token]\n",
    "            \n",
    "            # add list of characters and list of phonemes to lists\n",
    "            data_chars.append(list(clean_word))\n",
    "            data_phonemes.append(phonemes)\n",
    "            \n",
    "            # update list of all characters\n",
    "            for c in clean_word:\n",
    "                list_characters.add(c)\n",
    "            \n",
    "            # update list of all phonemes\n",
    "            for p in phonemes:\n",
    "                list_phonemes.add(p)\n",
    "                    \n",
    "list_characters = sorted(list(list_characters))\n",
    "list_phonemes = sorted(list(list_phonemes))\n",
    "\n",
    "num_samples = len(data_chars)\n",
    "\n",
    "num_encoder_tokens = len(list_characters)\n",
    "num_decoder_tokens = len(list_phonemes)\n",
    "\n",
    "max_encoder_seq_len = max([len(seq) for seq in data_chars])\n",
    "max_decoder_seq_len = max([len(seq) for seq in data_phonemes])\n",
    "\n",
    "print('Number of words before preprocessing:', len(list(words_all)))\n",
    "print('Number of pronunciations before preprocessing:', total_pronunciations)\n",
    "print('Number of words after preprocessing:', len(list(data_dict.values())))\n",
    "print('Number of pronunciations after preprocessing:', kept_pronunciations)\n",
    "print()\n",
    "\n",
    "print('Number of samples:', num_samples)\n",
    "print('Number of unique characters:', num_encoder_tokens)\n",
    "print('Number of unique phonemes:', num_decoder_tokens)\n",
    "print('Max sequence length for words:', max_encoder_seq_len)\n",
    "print('Max sequence length for pronunciations:', max_decoder_seq_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dict\n",
    "with open('data_dict.pickle', 'wb') as file_out:\n",
    "    pickle.dump(data_dict, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mappings\n",
    "def create_maps(elements):\n",
    "    return {e:i for i,e in enumerate(elements)}, {i:e for i,e in enumerate(elements)}\n",
    "\n",
    "map_char_id, map_id_char = create_maps(list_characters)\n",
    "map_phoneme_id, map_id_phoneme = create_maps(list_phonemes)\n",
    "\n",
    "# print(map_char_id)\n",
    "# print(map_id_char)\n",
    "# print(map_phoneme_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save mappings\n",
    "with open('mappings.pickle', 'wb') as file_out:\n",
    "    pickle.dump({'char_id':map_char_id, 'id_char':map_id_char,\n",
    "                 'phoneme_id':map_phoneme_id, 'id_phoneme':map_id_phoneme},\n",
    "                file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124558, 20, 26)\n",
      "(124558, 21, 71)\n"
     ]
    }
   ],
   "source": [
    "# convert character and phoneme data to 3D arrays\n",
    "encoder_input_data = np.zeros(\n",
    "    (num_samples, max_encoder_seq_len, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_len, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (num_samples, max_decoder_seq_len, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i_sample, (chars, phonemes) in enumerate(zip(data_chars, data_phonemes)):\n",
    "    for i_char, char in enumerate(chars):\n",
    "        encoder_input_data[i_sample, i_char, map_char_id[char]] = 1.\n",
    "    for i_phoneme, phoneme in enumerate(phonemes):\n",
    "        decoder_input_data[i_sample, i_phoneme, map_phoneme_id[phoneme]] = 1.\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "\n",
    "# print(decoder_target_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 3D arrays\n",
    "with open('data_preprocessed.pickle', 'wb') as file_out:\n",
    "    pickle.dump({'chars':encoder_input_data, 'phonemes':decoder_input_data}, file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTIDISESTABLISHMENTARIANISM\n",
      "DEINSTITUTIONALIZATION\n",
      "SUPERCALIFRAGILISTICEXPIALIDOCIOUS\n",
      "{0: 1, 1: 26, 2: 200, 3: 1529, 6: 21357, 8: 17756, 5: 14556, 7: 22120, 9: 13015, 4: 6533, 10: 8727, 11: 5071, 12: 2893, 13: 1555, 14: 691, 15: 348, 16: 140, 17: 64, 18: 21, 28: 1, 19: 6, 20: 6, 22: 1, 34: 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 24 artists>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPgklEQVR4nO3dW4xd5XnG8f9TO6WIhJSDQcimHZpYVQG1pFguUqqKija45cKkgspcBFeicoRASqRexOQmaSVLTtWEFqkgkYIwKAmxciiWKG0QSZRGQpAhouEUihVccGxhp6QJXITK5O3FvNNsD3PyzHj2Hub/k7b22u9a3/a7P/A8Xoe9JlWFJEm/NOwGJEmjwUCQJAEGgiSpGQiSJMBAkCS1tcNuYKHOPvvsGhsbG3YbkrSiPPHEEz+qqnXTrVuxgTA2Nsb4+Piw25CkFSXJf820zkNGkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAlbwN5V14sZ2PjjjugO7r1rGTiSNIvcQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpednp24SXlEpaLPcQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCS1OQMhyflJvpHkuSTPJPlI189M8nCSF/r5jIExtyTZn+T5JFcO1C9N8lSvuy1Jun5Kki92/bEkY0v/USVJs5nPHsIx4K+q6reAy4CbklwI7AQeqaqNwCP9ml63DbgI2ALcnmRNv9cdwA5gYz+2dP0G4MdV9V7gVuBTS/DZJEknYM5AqKrDVfXdXn4NeA5YD2wF9vRme4Cre3krcH9VvVFVLwL7gc1JzgNOr6pHq6qAe6eMmXyvLwFXTO49SJKWxwmdQ+hDOe8DHgPOrarDMBEawDm92Xrg5YFhB7u2vpen1o8bU1XHgJ8AZ51Ib5KkxZl3ICR5J/Bl4KNV9dPZNp2mVrPUZxsztYcdScaTjB89enSuliVJJ2BegZDkHUyEweeq6itdfqUPA9HPR7p+EDh/YPgG4FDXN0xTP25MkrXAu4FXp/ZRVXdW1aaq2rRu3br5tC5Jmqf5XGUU4C7guar6zMCqfcD2Xt4OPDBQ39ZXDl3AxMnjx/uw0mtJLuv3vH7KmMn3ugb4ep9nkCQtk/n8PoT3Ax8CnkryZNc+DuwG9ia5AXgJuBagqp5Jshd4lokrlG6qqjd73I3APcCpwEP9gInAuS/Jfib2DLYt8nNJkk7QnIFQVd9m+mP8AFfMMGYXsGua+jhw8TT1n9GBIkkaDr+pLEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIktp8fmOaVomxnQ/OuO7A7quWsRNJw+AegiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiRgHoGQ5O4kR5I8PVD7ZJIfJnmyH386sO6WJPuTPJ/kyoH6pUme6nW3JUnXT0nyxa4/lmRsaT+iJGk+5rOHcA+wZZr6rVV1ST/+BSDJhcA24KIec3uSNb39HcAOYGM/Jt/zBuDHVfVe4FbgUwv8LJKkRZgzEKrqW8Cr83y/rcD9VfVGVb0I7Ac2JzkPOL2qHq2qAu4Frh4Ys6eXvwRcMbn3IElaPos5h3Bzku/1IaUzurYeeHlgm4NdW9/LU+vHjamqY8BPgLOm+wOT7EgynmT86NGji2hdkjTVQgPhDuA9wCXAYeDTXZ/uX/Y1S322MW8tVt1ZVZuqatO6detOrGNJ0qwWFAhV9UpVvVlVPwc+C2zuVQeB8wc23QAc6vqGaerHjUmyFng38z9EJUlaIgsKhD4nMOmDwOQVSPuAbX3l0AVMnDx+vKoOA68luazPD1wPPDAwZnsvXwN8vc8zSJKW0dq5NkjyBeBy4OwkB4FPAJcnuYSJQzsHgA8DVNUzSfYCzwLHgJuq6s1+qxuZuGLpVOChfgDcBdyXZD8TewbbluKDSZJOzJyBUFXXTVO+a5btdwG7pqmPAxdPU/8ZcO1cfaxmYzsfnHHdgd1XLWMnkt7O/KayJAkwECRJzUCQJAEGgiSpGQiSJMBAkCQ1A0GSBBgIkqRmIEiSAANBktQMBEkSYCBIkpqBIEkC5nG3U2mQd16V3r7cQ5AkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1AwESRJgIEiSmoEgSQIMBElSMxAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwECQJDUDQZIEGAiSpGYgSJIAA0GS1OYMhCR3JzmS5OmB2plJHk7yQj+fMbDuliT7kzyf5MqB+qVJnup1tyVJ109J8sWuP5ZkbGk/oiRpPuazh3APsGVKbSfwSFVtBB7p1yS5ENgGXNRjbk+ypsfcAewANvZj8j1vAH5cVe8FbgU+tdAPI0lauDkDoaq+Bbw6pbwV2NPLe4CrB+r3V9UbVfUisB/YnOQ84PSqerSqCrh3ypjJ9/oScMXk3oMkafks9BzCuVV1GKCfz+n6euDlge0Odm19L0+tHzemqo4BPwHOmu4PTbIjyXiS8aNHjy6wdUnSdJb6pPJ0/7KvWeqzjXlrserOqtpUVZvWrVu3wBYlSdNZaCC80oeB6OcjXT8InD+w3QbgUNc3TFM/bkyStcC7eeshKknSSbbQQNgHbO/l7cADA/VtfeXQBUycPH68Dyu9luSyPj9w/ZQxk+91DfD1Ps8gSVpGa+faIMkXgMuBs5McBD4B7Ab2JrkBeAm4FqCqnkmyF3gWOAbcVFVv9lvdyMQVS6cCD/UD4C7gviT7mdgz2LYkn0ySdELmDISqum6GVVfMsP0uYNc09XHg4mnqP6MDRZI0PH5TWZIEGAiSpGYgSJIAA0GS1AwESRJgIEiS2pyXnUonamzngzOuO7D7qmXsRNKJcA9BkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLU1g67gdXOX0gvaVS4hyBJAgwESVIzECRJgIEgSWoGgiQJMBAkSc1AkCQBBoIkqRkIkiTAQJAktUUFQpIDSZ5K8mSS8a6dmeThJC/08xkD29+SZH+S55NcOVC/tN9nf5LbkmQxfUmSTtxS7CH8YVVdUlWb+vVO4JGq2gg80q9JciGwDbgI2ALcnmRNj7kD2AFs7MeWJehLknQCTsbN7bYCl/fyHuCbwMe6fn9VvQG8mGQ/sDnJAeD0qnoUIMm9wNXAQyehN42ImW7q5w39pOFZ7B5CAV9L8kSSHV07t6oOA/TzOV1fD7w8MPZg19b38tT6WyTZkWQ8yfjRo0cX2bokadBi9xDeX1WHkpwDPJzk+7NsO915gZql/tZi1Z3AnQCbNm2adhtJ0sIsag+hqg718xHgq8Bm4JUk5wH085He/CBw/sDwDcChrm+Ypi5JWkYLDoQkpyV51+Qy8AHgaWAfsL032w480Mv7gG1JTklyARMnjx/vw0qvJbmsry66fmCMJGmZLOaQ0bnAV/sK0bXA56vqX5N8B9ib5AbgJeBagKp6Jsle4FngGHBTVb3Z73UjcA9wKhMnkz2hLEnLbMGBUFU/AH5nmvp/A1fMMGYXsGua+jhw8UJ7kSQtnt9UliQBBoIkqRkIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLUDARJEmAgSJKagSBJAgwESVIzECRJgIEgSWqL/RWa0kkxtvPBGdcd2H3VMnYirR7uIUiSAANBktQMBEkSYCBIkpqBIEkCDARJUjMQJEmAgSBJagaCJAkwECRJzUCQJAHey+ik8548J49zKy0t9xAkSYCBIElqBoIkCTAQJEnNQJAkAQaCJKkZCJIkwO8h6G3O7ypI8+cegiQJMBAkSc1AkCQBI3QOIckW4B+ANcA/VdXuIbekVcBzDNIvjEQgJFkD/CPwx8BB4DtJ9lXVs8PtbHb+MFkd/O+s1WIkAgHYDOyvqh8AJLkf2AqMdCBIkwwNvR2kqobdA0muAbZU1V/26w8Bv1dVN0/Zbgewo1/+JvD8ErVwNvCjJXqv5bCS+l1JvYL9nkwrqVd4+/b761W1broVo7KHkGlqb0mqqroTuHPJ//BkvKo2LfX7niwrqd+V1CvY78m0knqF1dnvqFxldBA4f+D1BuDQkHqRpFVpVALhO8DGJBck+WVgG7BvyD1J0qoyEoeMqupYkpuBf2PistO7q+qZZWxhyQ9DnWQrqd+V1CvY78m0knqFVdjvSJxUliQN36gcMpIkDZmBIEkCVnkgJNmS5Pkk+5PsHHY/c0lyIMlTSZ5MMj7sfqZKcneSI0meHqidmeThJC/08xnD7HHQDP1+MskPe46fTPKnw+xxUpLzk3wjyXNJnknyka6P5PzO0u+ozu+vJHk8yX90v3/d9ZGb31l6XfTcrtpzCH27jP9k4HYZwHWjfLuMJAeATVU1kl+WSfIHwOvAvVV1cdf+Fni1qnZ36J5RVR8bZp+TZuj3k8DrVfV3w+xtqiTnAedV1XeTvAt4Arga+AtGcH5n6ffPGc35DXBaVb2e5B3At4GPAH/GiM3vLL1uYZFzu5r3EP7/dhlV9b/A5O0ytEBV9S3g1SnlrcCeXt7DxA+FkTBDvyOpqg5X1Xd7+TXgOWA9Izq/s/Q7kmrC6/3yHf0oRnB+Z+l10VZzIKwHXh54fZAR/h+2FfC1JE/0bTxWgnOr6jBM/JAAzhlyP/Nxc5Lv9SGloR8imCrJGPA+4DFWwPxO6RdGdH6TrEnyJHAEeLiqRnZ+Z+gVFjm3qzkQ5nW7jBHz/qr6XeBPgJv6kIeW1h3Ae4BLgMPAp4fbzvGSvBP4MvDRqvrpsPuZyzT9juz8VtWbVXUJE3dK2Jzk4mH3NJMZel303K7mQFhxt8uoqkP9fAT4KhOHvUbdK308efK48pEh9zOrqnql/7L9HPgsIzTHfbz4y8DnquorXR7Z+Z2u31Ge30lV9T/AN5k4Jj+y8wvH97oUc7uaA2FF3S4jyWl9co4kpwEfAJ6efdRI2Ads7+XtwAND7GVOk3/52wcZkTnuE4l3Ac9V1WcGVo3k/M7U7wjP77okv9rLpwJ/BHyfEZzfmXpdirldtVcZAfRlWX/PL26XsWvILc0oyW8wsVcAE7cc+fyo9ZvkC8DlTNyG9xXgE8A/A3uBXwNeAq6tqpE4kTtDv5czsctdwAHgw5PHkIcpye8D/w48Bfy8yx9n4rj8yM3vLP1ex2jO728zcdJ4DRP/UN5bVX+T5CxGbH5n6fU+Fjm3qzoQJEm/sJoPGUmSBhgIkiTAQJAkNQNBkgQYCJKkZiBIkgADQZLU/g9Tv/HLab+0jgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_len_freq = {}\n",
    "\n",
    "for word in all_words:\n",
    "    length = len(word)\n",
    "    \n",
    "    if length > 20:\n",
    "        print(word)\n",
    "    \n",
    "    if length not in word_len_freq:\n",
    "        word_len_freq[length] = 0\n",
    "    word_len_freq[length] += 1\n",
    "    \n",
    "print(word_len_freq)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(list(word_len_freq.keys()), list(word_len_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
